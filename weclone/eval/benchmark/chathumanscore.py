"""
ChatHumanScore Benchmark

Measures the "human-like" quality of conversational AI responses in casual chat scenarios.
Evaluates language naturalness, emotional alignment, diversity, context cohesion, and human signals.
"""

import re
import math
import asyncio
from typing import Any, Dict, List, Tuple, Optional
from collections import Counter
import json

from .base import BaseBenchmark, BenchmarkResult
from ..types import JobContext

try:
    import language_tool_python
    LANGUAGE_TOOL_AVAILABLE = True
except ImportError:
    LANGUAGE_TOOL_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    import torch
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


class ChatHumanScore(BaseBenchmark):
    """Evaluates human-like quality in casual conversations"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self._lang_tool = None
        self._sentence_model = None
        self._openai_client = None
        
        # Initialize tools lazily
        self._init_tools()
    
    def _init_tools(self):
        """Initialize NLP tools"""
        if LANGUAGE_TOOL_AVAILABLE and self.config.get("enable_grammar_check", True):
            try:
                self._lang_tool = language_tool_python.LanguageTool('zh-CN')
            except Exception:
                pass
        
        if SENTENCE_TRANSFORMERS_AVAILABLE and self.config.get("enable_semantic_analysis", True):
            try:
                model_name = self.config.get("sentence_model", "all-MiniLM-L6-v2")
                self._sentence_model = SentenceTransformer(model_name)
            except Exception:
                pass
        
        if OPENAI_AVAILABLE and self.config.get("enable_gpt_judge", True):
            api_key = self.config.get("openai_api_key")
            base_url = self.config.get("openai_base_url", "https://api.openai.com/v1")
            if api_key:
                try:
                    self._openai_client = openai.OpenAI(api_key=api_key, base_url=base_url)
                except Exception:
                    pass
    
    @property
    def _get_name(self) -> str:
        return "chathumanscore"
    
    def required_artifacts(self) -> List[str]:
        return ["conversation_text", "conversation_json"]
    
    def compute(self, job_ctx: JobContext) -> BenchmarkResult:
        """
        Compute ChatHumanScore metrics
        
        Metrics computed:
        - nat_score: Naturalness (0-1)
        - aff_score: Affective Alignment (0-1)  
        - div_score: Diversity (0-1)
        - ctx_score: Context Cohesion (0-1)
        - hum_score: Human Signal Strength (0-1)
        - score: Overall human-like score (0-10)
        - needs_human: Whether manual review is needed
        """
        if not self.validate_artifacts(job_ctx):
            return BenchmarkResult(
                benchmark_name=self.name,
                metrics=self._get_default_metrics()
            )
        
        conv_json = job_ctx.artifacts["conversation_json"]
        conv_text = job_ctx.artifacts["conversation_text"]
        
        # Extract assistant response(s)
        assistant_responses = self._extract_assistant_responses(conv_json)
        if not assistant_responses:
            return BenchmarkResult(
                benchmark_name=self.name,
                metrics=self._get_default_metrics()
            )
        
        # Combine responses for analysis
        response_text = " ".join(assistant_responses)
        
        # Basic filtering and penalty calculation
        penalties = self._basic_filter(response_text)
        
        # Compute individual metrics
        nat_score = self._compute_naturalness(response_text)
        aff_score = self._compute_affective_alignment(conv_json, assistant_responses)
        div_score = self._compute_diversity(assistant_responses)
        ctx_score = self._compute_context_cohesion(conv_json, assistant_responses)
        hum_score = self._compute_human_signals(response_text)
        
        # Apply penalties
        final_scores = {
            'nat_score': max(0.0, nat_score - penalties.get('grammar_penalty', 0)),
            'aff_score': max(0.0, aff_score - penalties.get('sentiment_penalty', 0)),
            'div_score': max(0.0, div_score - penalties.get('repetition_penalty', 0)),
            'ctx_score': max(0.0, ctx_score - penalties.get('context_penalty', 0)),
            'hum_score': max(0.0, hum_score - penalties.get('format_penalty', 0))
        }
        
        # Calculate weighted overall score (0-10 scale)
        weights = self.config.get('score_weights', {
            'naturalness': 0.25,
            'affective_alignment': 0.20,
            'diversity': 0.15,
            'context_cohesion': 0.20,
            'human_signal': 0.20
        })
        
        overall_score = (
            final_scores['nat_score'] * weights['naturalness'] +
            final_scores['aff_score'] * weights['affective_alignment'] +
            final_scores['div_score'] * weights['diversity'] +
            final_scores['ctx_score'] * weights['context_cohesion'] +
            final_scores['hum_score'] * weights['human_signal']
        ) * 10
        
        # Determine if human review is needed
        needs_human = (
            overall_score < self.config.get('human_review_threshold', 5.0) or
            any(score < 0.2 for score in final_scores.values())
        )
        
        metrics = {
            **final_scores,
            'score': round(overall_score, 2),
            'needs_human': needs_human,
            'penalty_total': sum(penalties.values())
        }
        
        metadata = {
            'conversation_id': job_ctx.conv_id,
            'response_count': len(assistant_responses),
            'response_length': len(response_text),
            'penalties': penalties,
            'weights_used': weights
        }
        
        return BenchmarkResult(
            benchmark_name=self.name,
            metrics=metrics,
            metadata=metadata
        )
    
    def _get_default_metrics(self) -> Dict[str, Any]:
        """Return default metrics for failed cases"""
        return {
            'nat_score': 0.0,
            'aff_score': 0.0,
            'div_score': 0.0,
            'ctx_score': 0.0,
            'hum_score': 0.0,
            'score': 0.0,
            'needs_human': True,
            'penalty_total': 1.0
        }
    
    def _extract_assistant_responses(self, conv_json: List[Dict]) -> List[str]:
        """Extract assistant responses from conversation"""
        responses = []
        for turn in conv_json:
            if turn.get('role') == 'assistant' and turn.get('content'):
                responses.append(turn['content'].strip())
        return responses
    
    def _basic_filter(self, text: str) -> Dict[str, float]:
        """Apply basic filtering and return penalties"""
        penalties = {}
        
        # Grammar error penalty
        if self._lang_tool and LANGUAGE_TOOL_AVAILABLE:
            try:
                errors = self._lang_tool.check(text)
                error_rate = len(errors) / max(len(text.split()), 1)
                max_error_rate = self.config.get('max_grammar_error_rate', 0.05)
                if error_rate > max_error_rate:
                    penalties['grammar_penalty'] = min(0.3, error_rate - max_error_rate)
            except Exception:
                pass
        
        # Repetition penalty
        words = text.split()
        if len(words) > 5:
            word_counts = Counter(words)
            total_words = len(words)
            repeated_words = sum(count - 1 for count in word_counts.values() if count > 1)
            repeat_ratio = repeated_words / total_words
            
            max_repeat_ratio = self.config.get('max_repeat_ratio', 0.30)
            if repeat_ratio > max_repeat_ratio:
                penalties['repetition_penalty'] = min(0.4, repeat_ratio - max_repeat_ratio)
        
        # Length penalties
        if len(text.strip()) < 2:
            penalties['format_penalty'] = 0.5
        elif len(text) > 1000:  # Too verbose
            penalties['format_penalty'] = 0.1
        
        return penalties
    
    def _compute_naturalness(self, text: str) -> float:
        """Compute naturalness score (0-1)"""
        if not text.strip():
            return 0.0
        
        score = 0.5  # Base score
        
        # Check for natural language patterns
        natural_patterns = self.config.get('natural_patterns', [
            r'[„ÄÇÔºÅÔºü]',  # Chinese punctuation
            r'[ÂïäÂëÄÂêßÂë¢]',  # Particles
            r'[ÂóØÂóØÂì¶Âìà]',  # Interjections
        ])
        
        for pattern in natural_patterns:
            if re.search(pattern, text):
                score += 0.1
        
        # Penalty for overly formal language
        formal_patterns = self.config.get('formal_patterns', [
            r'Ê†πÊçÆÊÇ®ÁöÑ|ÊåâÁÖßÊÇ®ÁöÑ',
            r'ÈùûÂ∏∏ÊÑüË∞¢ÊÇ®|ÊÑüË∞¢ÊÇ®ÁöÑ',
            r'ÊàëÂ∞Ü‰∏∫ÊÇ®|Êàë‰ºö‰∏∫ÊÇ®'
        ])
        
        for pattern in formal_patterns:
            if re.search(pattern, text):
                score -= 0.1
        
        return max(0.0, min(1.0, score))
    
    def _compute_affective_alignment(self, conv_json: List[Dict], responses: List[str]) -> float:
        """Compute emotional alignment score (0-1)"""
        if not responses:
            return 0.0
        
        # Simple sentiment analysis based on keywords
        positive_words = self.config.get('positive_words', ['Â•Ω', 'Ê£í', 'Ëµû', 'ÂºÄÂøÉ', 'È´òÂÖ¥'])
        negative_words = self.config.get('negative_words', ['‰∏çÂ•Ω', 'Á≥üÁ≥ï', 'ÈöæËøá', 'ÁîüÊ∞î', 'Â§±Êúõ'])
        
        # Get user sentiment from last user message
        user_sentiment = 0
        for turn in reversed(conv_json):
            if turn.get('role') == 'user' and turn.get('content'):
                user_text = turn['content']
                pos_count = sum(1 for word in positive_words if word in user_text)
                neg_count = sum(1 for word in negative_words if word in user_text)
                user_sentiment = pos_count - neg_count
                break
        
        # Check response sentiment
        response_text = " ".join(responses)
        pos_count = sum(1 for word in positive_words if word in response_text)
        neg_count = sum(1 for word in negative_words if word in response_text)
        response_sentiment = pos_count - neg_count
        
        # Calculate alignment
        if user_sentiment == 0 and response_sentiment == 0:
            return 0.7  # Neutral alignment
        elif user_sentiment * response_sentiment > 0:
            return 0.9  # Same direction
        elif user_sentiment * response_sentiment < 0:
            return 0.3  # Opposite direction
        else:
            return 0.6  # One neutral
    
    def _compute_diversity(self, responses: List[str]) -> float:
        """Compute diversity score (0-1)"""
        if not responses:
            return 0.0
        
        combined_text = " ".join(responses)
        words = combined_text.split()
        
        if len(words) < 3:
            return 0.3
        
        # Calculate distinct-n ratios
        unique_words = len(set(words))
        total_words = len(words)
        
        # Bigram diversity
        bigrams = [f"{words[i]}_{words[i+1]}" for i in range(len(words)-1)]
        unique_bigrams = len(set(bigrams))
        total_bigrams = max(len(bigrams), 1)
        
        diversity_score = 0.6 * (unique_words / total_words) + 0.4 * (unique_bigrams / total_bigrams)
        
        return min(1.0, diversity_score)
    
    def _compute_context_cohesion(self, conv_json: List[Dict], responses: List[str]) -> float:
        """Compute context cohesion score (0-1)"""
        if not responses or not self._sentence_model:
            return 0.7  # Default neutral score
        
        try:
            # Get last user message for context
            user_context = ""
            for turn in reversed(conv_json):
                if turn.get('role') == 'user' and turn.get('content'):
                    user_context = turn['content']
                    break
            
            if not user_context:
                return 0.7
            
            # Calculate semantic similarity
            response_text = " ".join(responses)
            embeddings = self._sentence_model.encode([user_context, response_text])
            
            # Cosine similarity
            cos_sim = torch.nn.functional.cosine_similarity(
                torch.tensor(embeddings[0]).unsqueeze(0),
                torch.tensor(embeddings[1]).unsqueeze(0)
            ).item()
            
            # Map similarity to score
            return max(0.0, min(1.0, (cos_sim + 1) / 2))
            
        except Exception:
            return 0.7
    
    def _compute_human_signals(self, text: str) -> float:
        """Compute human signal strength (0-1)"""
        if not text.strip():
            return 0.0
        
        score = 0.3  # Base score
        
        # Filler words (natural hesitation markers)
        filler_words = self.config.get('filler_words', ['ÂëÉ', 'ÂóØ', 'ÈÇ£‰∏™', 'Â∞±ÊòØ', 'ÂÖ∂ÂÆû'])
        filler_count = sum(1 for word in filler_words if word in text)
        score += min(0.3, filler_count * 0.1)
        
        # Emoji usage
        emoji_pattern = r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø‚ö†-‚ö°]'
        emoji_count = len(re.findall(emoji_pattern, text))
        score += min(0.2, emoji_count * 0.1)
        
        # Casual expressions
        casual_patterns = self.config.get('casual_patterns', [
            r'ÂìàÂìà|ÂòøÂòø|ÂëµÂëµ',
            r'Âêß[Ôºå„ÄÇÔºÅÔºü]',
            r'ÂëÄ[Ôºå„ÄÇÔºÅÔºü]',
        ])
        
        for pattern in casual_patterns:
            if re.search(pattern, text):
                score += 0.1
        
        # Personal references
        personal_patterns = ['ÊàëËßâÂæó', 'ÊàëÊÉ≥', 'ÊàëÊÑüËßâ', '‰∏™‰∫∫ËÆ§‰∏∫']
        personal_count = sum(1 for pattern in personal_patterns if pattern in text)
        score += min(0.2, personal_count * 0.1)
        
        return min(1.0, score)
    
    def get_description(self) -> str:
        return "Evaluates human-like quality in conversational responses across 5 dimensions"
    
    def get_config_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "enable_grammar_check": {
                    "type": "boolean",
                    "description": "Enable grammar checking",
                    "default": True
                },
                "enable_semantic_analysis": {
                    "type": "boolean", 
                    "description": "Enable semantic similarity analysis",
                    "default": True
                },
                "enable_gpt_judge": {
                    "type": "boolean",
                    "description": "Enable GPT-based judging",
                    "default": False
                },
                "openai_api_key": {
                    "type": "string",
                    "description": "OpenAI API key for GPT judge"
                },
                "openai_base_url": {
                    "type": "string",
                    "description": "OpenAI base URL",
                    "default": "https://api.openai.com/v1"
                },
                "sentence_model": {
                    "type": "string",
                    "description": "Sentence transformer model name",
                    "default": "all-MiniLM-L6-v2"
                },
                "max_grammar_error_rate": {
                    "type": "number",
                    "description": "Maximum acceptable grammar error rate",
                    "default": 0.05,
                    "minimum": 0
                },
                "max_repeat_ratio": {
                    "type": "number",
                    "description": "Maximum acceptable word repetition ratio",
                    "default": 0.30,
                    "minimum": 0
                },
                "human_review_threshold": {
                    "type": "number",
                    "description": "Score threshold below which human review is needed",
                    "default": 5.0,
                    "minimum": 0,
                    "maximum": 10
                },
                "score_weights": {
                    "type": "object",
                    "description": "Weights for each dimension",
                    "properties": {
                        "naturalness": {"type": "number", "default": 0.25},
                        "affective_alignment": {"type": "number", "default": 0.20},
                        "diversity": {"type": "number", "default": 0.15},
                        "context_cohesion": {"type": "number", "default": 0.20},
                        "human_signal": {"type": "number", "default": 0.20}
                    }
                },
                "filler_words": {
                    "type": "array",
                    "description": "List of filler words that indicate natural speech",
                    "items": {"type": "string"},
                    "default": ["ÂëÉ", "ÂóØ", "ÈÇ£‰∏™", "Â∞±ÊòØ", "ÂÖ∂ÂÆû"]
                },
                "positive_words": {
                    "type": "array",
                    "description": "List of positive sentiment words",
                    "items": {"type": "string"},
                    "default": ["Â•Ω", "Ê£í", "Ëµû", "ÂºÄÂøÉ", "È´òÂÖ¥"]
                },
                "negative_words": {
                    "type": "array",
                    "description": "List of negative sentiment words", 
                    "items": {"type": "string"},
                    "default": ["‰∏çÂ•Ω", "Á≥üÁ≥ï", "ÈöæËøá", "ÁîüÊ∞î", "Â§±Êúõ"]
                }
            },
            "additionalProperties": False
        } 