#!/usr/bin/env python3
"""
Export LoRA fine-tuned model to GGUF format for Ollama deployment.

This script merges LoRA adapters with base model, converts to GGUF format,
and creates necessary files for Ollama deployment.
"""

import os
import json
import shutil
import subprocess
import tempfile
from pathlib import Path
from typing import Optional
import commentjson

from weclone.utils.log import logger


def check_and_install_llama_cpp():
    """
    Check if llama.cpp tools are available, provide installation instructions if not.
    """
    # æ£€æŸ¥å„ç§å¯èƒ½çš„è½¬æ¢è„šæœ¬åç§°
    convert_scripts = [
        "convert-hf-to-gguf.py", 
        "convert_hf_to_gguf.py",
        "convert-hf-to-gguf",
        "convert_hf_to_gguf"
    ]
    
    convert_script = None
    for script_name in convert_scripts:
        script_path = shutil.which(script_name)
        if script_path:
            convert_script = script_path
            break
    
    # æ£€æŸ¥å„ç§å¯èƒ½çš„é‡åŒ–å·¥å…·åç§°
    quantize_tools = [
        "quantize", 
        "llama-quantize",
        "llamacpp-quantize"
    ]
    
    quantize_tool = None
    for tool_name in quantize_tools:
        tool_path = shutil.which(tool_name)
        if tool_path:
            quantize_tool = tool_path
            break
    
    if not convert_script:
        logger.error("æœªæ‰¾åˆ° llama.cpp è½¬æ¢å·¥å…·")
        logger.info("è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£… llama.cpp:")
        logger.info("=" * 60)
        logger.info("æ–¹æ³•1: ä½¿ç”¨ä¾¿æ·å®‰è£…è„šæœ¬ (æ¨è)")
        logger.info("   ./scripts/install_llama_cpp.sh")
        logger.info("")
        logger.info("æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½é¢„ç¼–è¯‘ç‰ˆæœ¬")
        logger.info("   1. è®¿é—®: https://github.com/ggerganov/llama.cpp/releases")
        logger.info("   2. ä¸‹è½½é€‚åˆæ‚¨ç³»ç»Ÿçš„é¢„ç¼–è¯‘ç‰ˆæœ¬")
        logger.info("   3. è§£å‹å¹¶å°†å·¥å…·æ·»åŠ åˆ° PATH")
        logger.info("")
        logger.info("æ–¹æ³•3: ä»æºç ç¼–è¯‘")
        logger.info("   git clone https://github.com/ggerganov/llama.cpp.git")
        logger.info("   cd llama.cpp && make")
        logger.info("   sudo ln -s $(pwd)/convert-hf-to-gguf.py /usr/local/bin/")
        logger.info("   sudo ln -s $(pwd)/quantize /usr/local/bin/")
        logger.info("=" * 60)
        return False
    
    if not quantize_tool:
        logger.warning("æœªæ‰¾åˆ°é‡åŒ–å·¥å…·ï¼Œå°†è·³è¿‡æ¨¡å‹é‡åŒ–æ­¥éª¤")
        logger.info("å¦‚éœ€é‡åŒ–åŠŸèƒ½ï¼Œè¯·ç¡®ä¿ llama.cpp çš„é‡åŒ–å·¥å…·åœ¨ PATH ä¸­")
    
    logger.info(f"æ‰¾åˆ°è½¬æ¢å·¥å…·: {convert_script}")
    if quantize_tool:
        logger.info(f"æ‰¾åˆ°é‡åŒ–å·¥å…·: {quantize_tool}")
    
    return True


def merge_lora_to_base_model(
    base_model_path: str,
    lora_adapter_path: str, 
    output_path: str,
    model_name: str = "qwen"
) -> None:
    """
    Merge LoRA adapter with base model using LLaMA Factory.
    """
    logger.info(f"æ­£åœ¨åˆå¹¶ LoRA adapter ({lora_adapter_path}) åˆ°åŸºç¡€æ¨¡å‹ ({base_model_path})")
    
    # Check if paths exist
    if not Path(base_model_path).exists():
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
    
    if not Path(lora_adapter_path).exists():
        raise FileNotFoundError(f"LoRA adapter è·¯å¾„ä¸å­˜åœ¨: {lora_adapter_path}")
    
    # Use LLaMA Factory's export functionality
    try:
        from llamafactory.train.tuner import export_model
        
        # Prepare arguments for model export
        export_args = [
            "--model_name_or_path", base_model_path,
            "--adapter_name_or_path", lora_adapter_path,
            "--template", model_name,
            "--finetuning_type", "lora",
            "--export_dir", output_path,
            "--export_size", "1",
            "--export_device", "cpu",
            "--trust_remote_code", "True"
        ]
        
        # Export merged model
        original_argv = os.sys.argv.copy()
        try:
            os.sys.argv = ["export_model.py"] + export_args
            export_model()
            logger.info(f"æ¨¡å‹åˆå¹¶å®Œæˆï¼Œè¾“å‡ºè·¯å¾„: {output_path}")
        finally:
            os.sys.argv = original_argv
            
    except ImportError as e:
        logger.error("å¯¼å…¥ LLaMA Factory å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {e}")
        raise
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆå¹¶å¤±è´¥: {e}")
        raise


def convert_to_gguf(
    merged_model_path: str,
    output_gguf_path: str,
    quantization: str = "q4_k_m"
) -> None:
    """
    Convert merged model to GGUF format using llama.cpp tools.
    """
    logger.info(f"æ­£åœ¨å°†æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼...")
    
    if not Path(merged_model_path).exists():
        raise FileNotFoundError(f"åˆå¹¶åçš„æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {merged_model_path}")
    
    try:
        # æŸ¥æ‰¾è½¬æ¢è„šæœ¬
        convert_scripts = [
            "convert-hf-to-gguf.py", 
            "convert_hf_to_gguf.py",
            "convert-hf-to-gguf",
            "convert_hf_to_gguf"
        ]
        
        convert_script = None
        for script_name in convert_scripts:
            script_path = shutil.which(script_name)
            if script_path:
                convert_script = script_path
                break
        
        if not convert_script:
            raise FileNotFoundError("æœªæ‰¾åˆ° llama.cpp è½¬æ¢å·¥å…·")
        
        # Create output directory
        Path(output_gguf_path).parent.mkdir(parents=True, exist_ok=True)
        
        # Convert to GGUF (f32 format first)
        temp_gguf_path = output_gguf_path.replace(".gguf", "_f32.gguf")
        
        # è·å–ç»å¯¹è·¯å¾„
        abs_merged_path = str(Path(merged_model_path).absolute())
        abs_gguf_path = str(Path(temp_gguf_path).absolute())
        
        # æ„å»ºè½¬æ¢å‘½ä»¤
        if convert_script.endswith('.py'):
            convert_cmd = [
                "python", convert_script,
                abs_merged_path,
                "--outfile", abs_gguf_path,
                "--outtype", "f32"
            ]
        else:
            # å¦‚æœæ˜¯äºŒè¿›åˆ¶æ–‡ä»¶
            convert_cmd = [
                convert_script,
                abs_merged_path,
                "--outfile", abs_gguf_path,
                "--outtype", "f32"
            ]
        
        logger.info(f"æ‰§è¡Œè½¬æ¢å‘½ä»¤: {' '.join(convert_cmd)}")
        result = subprocess.run(convert_cmd, capture_output=True, text=True, cwd=Path(output_gguf_path).parent)
        
        if result.returncode != 0:
            logger.error(f"GGUF è½¬æ¢å¤±è´¥:")
            logger.error(f"stdout: {result.stdout}")
            logger.error(f"stderr: {result.stderr}")
            raise RuntimeError(f"GGUF conversion failed: {result.stderr}")
        
        logger.info("GGUF åŸºç¡€è½¬æ¢å®Œæˆ")
        
        # Quantize the model if tools available
        quantize_tools = ["quantize", "llama-quantize", "llamacpp-quantize"]
        quantize_tool = None
        for tool_name in quantize_tools:
            tool_path = shutil.which(tool_name)
            if tool_path:
                quantize_tool = tool_path
                break
        
        if quantize_tool and quantization != "f32":
            logger.info(f"æ­£åœ¨é‡åŒ–æ¨¡å‹: {quantization}")
            quantize_cmd = [
                quantize_tool,
                temp_gguf_path,
                output_gguf_path,
                quantization
            ]
            
            result = subprocess.run(quantize_cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.warning(f"é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ f32 æ¨¡å‹:")
                logger.warning(f"stderr: {result.stderr}")
                shutil.move(temp_gguf_path, output_gguf_path)
            else:
                # Remove the temporary f32 file
                if Path(temp_gguf_path).exists():
                    os.remove(temp_gguf_path)
                logger.info(f"é‡åŒ–å®Œæˆ: {output_gguf_path}")
        else:
            # No quantization, just rename the f32 file
            shutil.move(temp_gguf_path, output_gguf_path)
            if quantize_tool is None:
                logger.info("æœªæ‰¾åˆ°é‡åŒ–å·¥å…·ï¼Œä¿å­˜ä¸º f32 æ ¼å¼")
            else:
                logger.info(f"GGUF è½¬æ¢å®Œæˆ (f32): {output_gguf_path}")
            
    except Exception as e:
        logger.error(f"GGUF è½¬æ¢å¤±è´¥: {e}")
        raise


def create_ollama_modelfile(
    gguf_path: str,
    output_dir: str,
    model_name: str = "weclone-qwen",
    system_prompt: Optional[str] = None,
    temperature: float = 0.5,
    top_p: float = 0.65,
    repeat_penalty: float = 1.2
) -> str:
    """
    Create Ollama Modelfile for the GGUF model.
    """
    modelfile_path = Path(output_dir) / "Modelfile"
    
    # Default system prompt
    if system_prompt is None:
        system_prompt = "è¯·ä½ æ‰®æ¼”ä¸€åäººç±»ï¼Œä¸è¦è¯´è‡ªå·±æ˜¯äººå·¥æ™ºèƒ½"
    
    modelfile_content = f"""FROM {Path(gguf_path).name}

# Set system prompt
SYSTEM \"\"\"{system_prompt}\"\"\"

# Set model parameters
PARAMETER temperature {temperature}
PARAMETER top_p {top_p}
PARAMETER repeat_penalty {repeat_penalty}
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

# Template for Qwen model
TEMPLATE \"\"\"{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
{{{{ end }}}}{{{{ .Response }}}}<|im_end|>
\"\"\"
"""
    
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    logger.info(f"Ollama Modelfile å·²åˆ›å»º: {modelfile_path}")
    return str(modelfile_path)


def create_deployment_script(
    output_dir: str,
    model_name: str = "weclone-qwen",
    gguf_filename: str = "model.gguf"
) -> str:
    """
    Create deployment script for Windows Ollama.
    """
    script_path = Path(output_dir) / "deploy_to_ollama.bat"
    
    script_content = f"""@echo off
echo æ­£åœ¨éƒ¨ç½² WeClone æ¨¡å‹åˆ° Ollama...

REM æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…
ollama --version >nul 2>&1
if %errorlevel% neq 0 (
    echo é”™è¯¯: æœªæ‰¾åˆ° Ollamaï¼Œè¯·å…ˆå®‰è£… Ollama
    echo ä¸‹è½½åœ°å€: https://ollama.ai/download/windows
    pause
    exit /b 1
)

REM åˆ›å»ºæ¨¡å‹
echo æ­£åœ¨åˆ›å»º Ollama æ¨¡å‹: {model_name}
ollama create {model_name} -f Modelfile

if %errorlevel% equ 0 (
    echo æ¨¡å‹åˆ›å»ºæˆåŠŸï¼
    echo.
    echo ä½¿ç”¨æ–¹æ³•:
    echo   ollama run {model_name}
    echo.
    echo æˆ–è€…åœ¨å…¶ä»–åº”ç”¨ä¸­ä½¿ç”¨ API:
    echo   curl http://localhost:11434/api/generate -d "{{\"model\":\"{model_name}\",\"prompt\":\"ä½ å¥½\"}}"
) else (
    echo æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´
)

pause
"""
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    logger.info(f"éƒ¨ç½²è„šæœ¬å·²åˆ›å»º: {script_path}")
    return str(script_path)


def create_readme(output_dir: str, model_name: str = "weclone-qwen") -> str:
    """
    Create README file with deployment instructions.
    """
    readme_path = Path(output_dir) / "README_DEPLOYMENT.md"
    
    readme_content = f"""# WeClone Ollama éƒ¨ç½²è¯´æ˜

## æ–‡ä»¶è¯´æ˜

- `model.gguf`: è½¬æ¢åçš„ GGUF æ ¼å¼æ¨¡å‹æ–‡ä»¶
- `Modelfile`: Ollama æ¨¡å‹é…ç½®æ–‡ä»¶
- `deploy_to_ollama.bat`: Windows éƒ¨ç½²è„šæœ¬
- `README_DEPLOYMENT.md`: æœ¬è¯´æ˜æ–‡ä»¶

## éƒ¨ç½²æ­¥éª¤

### 1. å®‰è£… Ollama (Windows)

å¦‚æœè¿˜æœªå®‰è£… Ollamaï¼Œè¯·ä»å®˜ç½‘ä¸‹è½½å®‰è£…ï¼š
https://ollama.ai/download/windows

### 2. éƒ¨ç½²æ¨¡å‹

å°†æ•´ä¸ªæ–‡ä»¶å¤¹å¤åˆ¶åˆ° Windows ç³»ç»Ÿä¸­ï¼Œç„¶åï¼š

**æ–¹æ³•ä¸€ï¼šä½¿ç”¨éƒ¨ç½²è„šæœ¬**
åŒå‡»è¿è¡Œ `deploy_to_ollama.bat`

**æ–¹æ³•äºŒï¼šæ‰‹åŠ¨éƒ¨ç½²**
```cmd
cd /path/to/this/folder
ollama create {model_name} -f Modelfile
```

### 3. ä½¿ç”¨æ¨¡å‹

**å‘½ä»¤è¡ŒèŠå¤©ï¼š**
```cmd
ollama run {model_name}
```

**API è°ƒç”¨ï¼š**
```bash
curl http://localhost:11434/api/generate \\
  -d '{{"model":"{model_name}","prompt":"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}}'
```

**Python è°ƒç”¨ç¤ºä¾‹ï¼š**
```python
import requests

response = requests.post('http://localhost:11434/api/generate', 
    json={{"model": "{model_name}", "prompt": "ä½ å¥½"}})
print(response.json())
```

## å‚æ•°è°ƒæ•´

å¦‚éœ€è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œå¯ä»¥ç¼–è¾‘ `Modelfile` æ–‡ä»¶ä¸­çš„ PARAMETER éƒ¨åˆ†ï¼š

- `temperature`: æ§åˆ¶è¾“å‡ºéšæœºæ€§ (0.1-2.0)
- `top_p`: æ ¸é‡‡æ ·å‚æ•° (0.1-1.0) 
- `repeat_penalty`: é‡å¤æƒ©ç½š (1.0-1.5)

ä¿®æ”¹åé‡æ–°è¿è¡Œéƒ¨ç½²å‘½ä»¤å³å¯ã€‚

## æ•…éšœæ’é™¤

1. **æ¨¡å‹æ–‡ä»¶è¿‡å¤§**: GGUF æ–‡ä»¶å¯èƒ½å¾ˆå¤§ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿç£ç›˜ç©ºé—´
2. **æƒé™é—®é¢˜**: åœ¨ Windows ä¸Šå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™è¿è¡Œ
3. **å†…å­˜ä¸è¶³**: 7B æ¨¡å‹å¤§çº¦éœ€è¦ 4-8GB RAM

## æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·è®¿é—® WeClone é¡¹ç›®é¡µé¢ï¼š
https://github.com/xming521/WeClone
"""
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    logger.info(f"éƒ¨ç½²è¯´æ˜å·²åˆ›å»º: {readme_path}")
    return str(readme_path)


def main():
    """
    Main function to export LoRA model to GGUF format for Ollama.
    """
    logger.info("å¼€å§‹å¯¼å‡º WeClone æ¨¡å‹åˆ° GGUF æ ¼å¼...")
    
    # Check dependencies first
    if not check_and_install_llama_cpp():
        logger.error("ç¼ºå°‘å¿…è¦çš„ä¾èµ–ï¼Œæ— æ³•ç»§ç»­å¯¼å‡º")
        logger.info("è¯·æŒ‰ç…§ä¸Šè¿°è¯´æ˜å®‰è£… llama.cpp å·¥å…·åå†è¯•")
        return
    
    # Load configuration
    config_path = "./settings.jsonc"
    with open(config_path, "r", encoding="utf-8") as f:
        all_config = commentjson.load(f)
    
    common_args = all_config.get("common_args", {})
    infer_args = all_config.get("infer_args", {})
    
    # Get paths from config
    base_model_path = common_args.get("model_name_or_path", "./Qwen2.5-7B-Instruct")
    lora_adapter_path = common_args.get("adapter_name_or_path", "./model_output")
    template = common_args.get("template", "qwen")
    system_prompt = common_args.get("default_system", "è¯·ä½ æ‰®æ¼”ä¸€åäººç±»ï¼Œä¸è¦è¯´è‡ªå·±æ˜¯äººå·¥æ™ºèƒ½")
    
    # Get inference parameters
    temperature = infer_args.get("temperature", 0.5)
    top_p = infer_args.get("top_p", 0.65)
    repeat_penalty = infer_args.get("repetition_penalty", 1.2)
    
    # Validate paths
    logger.info(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
    logger.info(f"LoRA adapter è·¯å¾„: {lora_adapter_path}")
    
    # Create output directory
    output_dir = Path("./ollama_export")
    output_dir.mkdir(exist_ok=True)
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    try:
        # Step 1: Merge LoRA with base model
        merged_model_dir = output_dir / "merged_model"
        merge_lora_to_base_model(
            base_model_path=base_model_path,
            lora_adapter_path=lora_adapter_path,
            output_path=str(merged_model_dir),
            model_name=template
        )
        
        # Step 2: Convert to GGUF
        gguf_path = output_dir / "model.gguf"
        convert_to_gguf(
            merged_model_path=str(merged_model_dir),
            output_gguf_path=str(gguf_path),
            quantization="q4_k_m"  # Good balance of size and quality
        )
        
        # Step 3: Create Ollama Modelfile
        modelfile_path = create_ollama_modelfile(
            gguf_path=str(gguf_path),
            output_dir=str(output_dir),
            model_name="weclone-qwen",
            system_prompt=system_prompt,
            temperature=temperature,
            top_p=top_p,
            repeat_penalty=repeat_penalty
        )
        
        # Step 4: Create deployment script
        script_path = create_deployment_script(
            output_dir=str(output_dir),
            model_name="weclone-qwen"
        )
        
        # Step 5: Create README
        readme_path = create_readme(
            output_dir=str(output_dir),
            model_name="weclone-qwen"
        )
        
        # Clean up merged model directory (optional, to save space)
        if merged_model_dir.exists():
            shutil.rmtree(merged_model_dir)
            logger.info("å·²æ¸…ç†ä¸´æ—¶åˆå¹¶æ¨¡å‹æ–‡ä»¶")
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ GGUF å¯¼å‡ºå®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
        logger.info(f"ğŸ“¦ GGUF æ¨¡å‹: {gguf_path}")
        logger.info(f"âš™ï¸  Modelfile: {modelfile_path}")
        logger.info(f"ğŸš€ éƒ¨ç½²è„šæœ¬: {script_path}")
        logger.info(f"ğŸ“– è¯´æ˜æ–‡æ¡£: {readme_path}")
        logger.info("")
        logger.info("ä¸‹ä¸€æ­¥:")
        logger.info("1. å°†æ•´ä¸ª ollama_export æ–‡ä»¶å¤¹å¤åˆ¶åˆ° Windows ç³»ç»Ÿ")
        logger.info("2. åœ¨ Windows ä¸Šè¿è¡Œ deploy_to_ollama.bat")
        logger.info("3. ä½¿ç”¨ 'ollama run weclone-qwen' å¼€å§‹èŠå¤©")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main() 